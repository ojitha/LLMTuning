{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1d87d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ojitha/GitHub/LLMTuning/sagemaker_examples/.venv/lib/python3.12/site-packages/sagemaker_core/main/shapes.py:6645: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  domain: The machine learning domain of the model and its components. Valid Values: COMPUTER_VISION \\| NATURAL_LANGUAGE_PROCESSING \\| MACHINE_LEARNING\n",
      "/Users/ojitha/GitHub/LLMTuning/sagemaker_examples/.venv/lib/python3.12/site-packages/sagemaker_core/main/shapes.py:7450: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  schedule_expression: A cron expression that describes details about the monitoring schedule. The supported cron expressions are:   If you want to set the job to start every hour, use the following:  Hourly: cron(0 \\* ? \\* \\* \\*)    If you want to start the job daily:  cron(0 [00-23] ? \\* \\* \\*)    If you want to run the job one time, immediately, use the following keyword:  NOW    For example, the following are valid cron expressions:   Daily at noon UTC: cron(0 12 ? \\* \\* \\*)    Daily at midnight UTC: cron(0 0 ? \\* \\* \\*)    To support running every 6, 12 hours, the following are also supported:  cron(0 [00-23]/[01-24] ? \\* \\* \\*)  For example, the following are valid cron expressions:   Every 12 hours, starting at 5pm UTC: cron(0 17/12 ? \\* \\* \\*)    Every two hours starting at midnight: cron(0 0/2 ? \\* \\* \\*)       Even though the cron expression is set to start at 5PM UTC, note that there could be a delay of 0-20 minutes from the actual requested time to run the execution.    We recommend that if you would like a daily schedule, you do not provide this parameter. Amazon SageMaker AI will pick a time for running every day.    You can also specify the keyword NOW to run the monitoring job immediately, one time, without recurring.\n",
      "/Users/ojitha/GitHub/LLMTuning/sagemaker_examples/.venv/lib/python3.12/site-packages/sagemaker_core/main/shapes.py:9820: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  resource_retained_billable_time_in_seconds: The billable time in seconds used by the warm pool. Billable time refers to the absolute wall-clock time. Multiply ResourceRetainedBillableTimeInSeconds by the number of instances (InstanceCount) in your training cluster to get the total compute time SageMaker bills you if you run warm pool training. The formula is as follows: ResourceRetainedBillableTimeInSeconds \\* InstanceCount.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/ojitha/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ojitha/GitHub/LLMTuning/sagemaker_examples/.venv/lib/python3.12/site-packages/smdebug_rulesconfig/actions/utils.py:5: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  TRAINING_JOB_PREFIX_REGEX = \"^[A-Za-z0-9\\-]+$\"\n",
      "/Users/ojitha/GitHub/LLMTuning/sagemaker_examples/.venv/lib/python3.12/site-packages/smdebug_rulesconfig/actions/utils.py:6: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  EMAIL_ADDRESS_REGEX = \"^[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"\n",
      "/Users/ojitha/GitHub/LLMTuning/sagemaker_examples/.venv/lib/python3.12/site-packages/smdebug_rulesconfig/actions/utils.py:7: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  PHONE_NUMBER_REGEX = \"^\\+\\d{1,15}$\"\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c51599af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"1.2.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58e1bf",
   "metadata": {},
   "source": [
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b8dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_session(local_download_dir) -> sagemaker.Session:\n",
    "    \"\"\"Return the SageMaker session.\"\"\"\n",
    "\n",
    "    sagemaker_client = boto3.client(\n",
    "        service_name=\"sagemaker\", region_name=boto3.Session().region_name\n",
    "    )\n",
    "\n",
    "    session_settings = sagemaker.session_settings.SessionSettings(\n",
    "        local_download_dir=local_download_dir\n",
    "    )\n",
    "\n",
    "    # the unit test will ensure you do not commit this change\n",
    "    session = sagemaker.session.Session(\n",
    "        sagemaker_client=sagemaker_client, settings=session_settings\n",
    "    )\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa2bc4",
   "metadata": {},
   "source": [
    "This text-to-text generation task supports a wide variety of model sizes that have different compute requirements. Here, we specify the instance type for several large models along with an environment variable to set the multi-model endpoint number of workers to 1. This ensures we can support the largest possible token lengths since additional models are not consuming GPU memory resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20fd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE CELL 6 ##\n",
    "_large_model_env = {\"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\", \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"}\n",
    "\n",
    "_model_config_map = {\n",
    "    \"huggingface-text2text-flan-t5-xxl\": {\n",
    "        \"instance_type\": \"ml.g5.12xlarge\",\n",
    "        \"env\": _large_model_env,\n",
    "    },\n",
    "    \"huggingface-text2text-flan-t5-xxl-fp16\": {\n",
    "        \"instance_type\": \"ml.g5.12xlarge\",\n",
    "        \"env\": _large_model_env,\n",
    "    },\n",
    "    \"huggingface-text2text-t5-one-line-summary\": {\n",
    "        \"instance_type\": \"ml.g5.2xlarge\",\n",
    "        \"env\": _large_model_env,\n",
    "    },\n",
    "    \"huggingface-text2text-flan-t5-xl\": {\n",
    "        \"instance_type\": \"ml.g5.2xlarge\",\n",
    "        \"env\": {\"MMS_DEFAULT_WORKERS_PER_MODEL\": \"1\"},\n",
    "    },\n",
    "    \"huggingface-text2text-flan-t5-large\": {\n",
    "        \"instance_type\": \"ml.g5.2xlarge\",\n",
    "        \"env\": {\"MMS_DEFAULT_WORKERS_PER_MODEL\": \"1\"},\n",
    "    },\n",
    "    \"huggingface-text2text-flan-ul2-bf16\": {\n",
    "        \"instance_type\": \"ml.g5.12xlarge\",\n",
    "        \"env\": _large_model_env,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d4ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "if model_id in _model_config_map:\n",
    "    inference_instance_type = _model_config_map[model_id][\"instance_type\"]\n",
    "else:\n",
    "    inference_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d710d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://jumpstart-cache-prod-ap-southeast-2/huggingface-infer/prepack/v1.1.2/infer-prepack-huggingface-text2text-flan-t5-xl.tar.gz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc61f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# Create the SageMaker model instance\n",
    "if model_id in _model_config_map:\n",
    "    # For those large models, we already repack the inference script and model\n",
    "    # artifacts for you, so the `source_dir` argument to Model is not required.\n",
    "    model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        model_data=model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=endpoint_name,\n",
    "        env=_model_config_map[model_id][\"env\"],\n",
    "    )\n",
    "else:\n",
    "    model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        source_dir=deploy_source_uri,\n",
    "        model_data=model_uri,\n",
    "        entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=endpoint_name,\n",
    "        sagemaker_session=get_sagemaker_session(\"download_dir\"),\n",
    "    )\n",
    "\n",
    "# Deploy the Model. Note that we need to pass Predictor class when we deploy model through the Model class\n",
    "# defined above, so we can run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2b2f7",
   "metadata": {},
   "source": [
    "Input to the endpoint is any string of text formatted as json and encoded in `utf-8` format. Output of the endpoint is a `json` with generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ece6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-text\", Body=encoded_text\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d9627",
   "metadata": {},
   "source": [
    "Below, we put in some example input text. You can put in any text and the model predicts next words in the sequence. Longer sequences of text can be generated by calling the model repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec32a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate to German:  My name is Arthur\n",
      "generated text: \u001b[1mIch bin Arthur.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: Recipe: How to make bolognese pasta\n",
      "generated text: \u001b[1mBring a large pot of water to a boil. Add the pasta and cook according to\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: Summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overtons vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.\n",
      "generated text: \u001b[1mWe describe a system called Overton, whose main design goal is to support engineers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "text1 = \"Translate to German:  My name is Arthur\"\n",
    "text2 = \"Recipe: How to make bolognese pasta\"\n",
    "text3 = \"Summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production \\\n",
    "machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and \\\n",
    "handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a \\\n",
    "set of novel high-level, declarative abstractions. Overtons vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. \\\n",
    "In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, \\\n",
    "Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, \\\n",
    "Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.\"\n",
    "\n",
    "f = open(\"diy_results.txt\",\"w\")\n",
    "f.write(f\"{endpoint_name}{newline}\")\n",
    "\n",
    "for text in [text1, text2, text3]:\n",
    "    query_response = query_endpoint(text.encode(\"utf-8\"), endpoint_name=endpoint_name)\n",
    "    generated_text = parse_response(query_response)\n",
    "    f.write(f\"generated text: {generated_text}{newline}\")\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"input text: {text}{newline}\"\n",
    "        f\"generated text: {bold}{generated_text}{unbold}{newline}\"\n",
    "    )\n",
    "    \n",
    "f.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e342c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "by using the huggingface-text2text-t5-one-line-summary LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9s2zlvbyjr",
   "metadata": {},
   "source": [
    "## Cleanup AWS Resources\n",
    "\n",
    "To avoid ongoing charges, it's important to clean up the AWS resources created during this notebook execution. This includes deleting the SageMaker endpoint and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cw38iipq9x5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted endpoint: jumpstart-example-huggingface-text2text-2025-09-20-03-30-36-375\n",
      "Successfully deleted model: jumpstart-example-huggingface-text2text-2025-09-20-03-30-36-375\n",
      "Cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "try:\n",
    "    model_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(f\"Successfully deleted endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint {endpoint_name}: {str(e)}\")\n",
    "\n",
    "# Delete the SageMaker model\n",
    "try:\n",
    "    model.delete_model()\n",
    "    print(f\"Successfully deleted model: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model {endpoint_name}: {str(e)}\")\n",
    "\n",
    "print(\"Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48146f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
