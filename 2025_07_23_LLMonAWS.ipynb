{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+mwHx6iT5uG82HbsedjwA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "layout: post\n",
        "title:  LLM on AWS\n",
        "date:   2025-07-23\n",
        "maths: true\n",
        "categories: [AI, AWS]\n",
        "typora-root-url: /Users/ojitha/GitHub/ojitha.github.io\n",
        "typora-copy-images-to: ../../blog/assets/images/${filename}\n",
        "---\n",
        "\n",
        "Breif introduction\n",
        "\n",
        "<!--more-->\n",
        "\n",
        "------\n",
        "\n",
        "* TOC\n",
        "{:toc}\n",
        "------\n",
        "\n",
        "## LLMs\n",
        "\n",
        "Language Models use statistical methods to predict the succession of following tokens in a text sequence. Generative Al is a technique that uses neural networks to generate next words in a sequence\n",
        "\n",
        "The conditional probability eauation:\n",
        "\n",
        "$$P\\left( x^{\\left( t+1\\right) }| x^{\\left( t\\right) },\\ldots ,x^{\\left( 1\\right) }\\right)$$\n",
        "\n",
        "**next in sequence** is the left of the `|` and right side shows **history**. An n-gram is a contiguous sequence of words (tokens) from a given text sample. The model uses a fixed-size window of previous words to calculate the probability of the next word.\n",
        "\n",
        "### N-grams and LLM Tokenizers\n",
        "\n",
        "An **n-gram** is a contiguous sequence of n items from a text or speech sample. In the context of LLMs and tokenizers, these \"items\" are typically characters or bytes.\n",
        "\n",
        "#### How N-grams Relate to Tokenization\n",
        "\n",
        "Modern LLM tokenizers like **BPE (Byte Pair Encoding)** and **WordPiece** are fundamentally based on n-gram principles:\n",
        "\n",
        "Character-Level N-grams\n",
        "\n",
        "- **Unigram (1-gram)**: Single characters â†’ `\"h\"`, `\"e\"`, `\"l\"`, `\"l\"`, `\"o\"`\n",
        "- **Bigram (2-gram)**: Character pairs â†’ `\"he\"`, `\"el\"`, `\"ll\"`, `\"lo\"`\n",
        "- **Trigram (3-gram)**: Character triples â†’ `\"hel\"`, `\"ell\"`, `\"llo\"`\n",
        "\n",
        "How Tokenizers Use This\n",
        "\n",
        "**BPE[^3] (used by GPT models):**\n",
        "\n",
        "1. Starts with individual characters (unigrams)\n",
        "2. Iteratively merges the most frequent adjacent pairs\n",
        "3. Builds a vocabulary of increasingly longer n-grams\n",
        "4. Example: `\"hello\"` might become `[\"he\", \"llo\"]` or `[\"hello\"]` depending on frequency\n",
        "\n",
        "**Key insight**: The tokenizer learns which n-grams appear most frequently in training data and treats them as single tokens. Common words become single tokens, while rare words get split into subword n-grams.\n",
        "\n",
        "Why This Matters\n",
        "\n",
        "- **Efficiency**: Frequent n-grams (like \"the\", \"ing\") become single tokens\n",
        "- **Flexibility**: Rare words decompose into known subword n-grams\n",
        "- **Vocabulary size**: Balances between character-level (too many tokens per word) and word-level (vocabulary too large)\n",
        "\n",
        "The vocabulary of modern tokenizers is essentially a learned dictionary of the most useful n-grams from 1 up to some maximum length (often whole words). In the prompt, the input text is break into tokens. Tokens are converted into a vector (word embeddings) of that model.\n",
        "\n",
        "Transformer model architecture consists of encoder and decoder:\n",
        "\n",
        "- Encoder processes (understand) tokens\n",
        "    - Auto-encoder models (BERT, etc.)\n",
        "- Decoder generates tokens\n",
        "    - Auto-regressive models (GPT, Claude, etc.)\n",
        "\n",
        "> When you say parameters in the LLM, tha mean weights.\n",
        "{:.info-box}\n",
        "\n",
        "In a Feed Forward Network(FNN), the Weights are multiplied by the incoming values, then summed up and passed to an activation function (ReLU): $$f\\left( \\sum w_{i}x_{i}+b\\right) $$\n",
        "\n",
        "### Positional Encoding in Transformers\n",
        "\n",
        "The position of the word of the sequence is maintained by Positional encoding in the parallel process in the encoder. Positional encoding is a technique that allows Transformers to understand the **order** of tokens in a sequence, since the attention mechanism itself is position-agnostic. The offsets must be really small, to ensure the semantic representation of the token doesn't change. Unlike RNNs that process tokens sequentially, *Transformers process all tokens in parallel*. Without positional encoding, the sentence \"The cat chased the mouse\" would be identical to \"The mouse chased the cat\" to the model. Many newer models (like GPT-3, GPT-4) use **learned positional embeddings** instead, which are trained alongside the model rather than using fixed sinusoids.\n",
        "\n",
        "### Self-attention\n",
        "\n",
        "Self-attention is the core mechanism that allows Transformers to understand relationships between all words in a sentence simultaneously. Unlike reading sequentially, self-attention lets each word \"look at\" every other word and decide which ones are most relevant to understanding its meaning.\n",
        "\n",
        "Here's how it works: For each word, the model creates three vectors called **Query** (Q), **Key** (K), and **Value** (V).\n",
        "\n",
        "- **Query** Vector: Q is a set of \"questions\" that each token asks about each other token.\n",
        "- **Key** Vector: the K vector helps determine the importance of each part of the input relative to the query.\n",
        "- **Value** Vector: The V (value) vector represents the actual data or information to be attended.\n",
        "\n",
        "Think of it like a search system - the Query is \"what am I looking for?\", the Key is \"what do I contain?\", and the Value is \"what information do I offer?\". When processing a word, its Query vector is compared against the Key vectors of all other words (including itself) through a dot product operation. This comparison produces attention scores that indicate how much focus should be placed on each word. These scores are then normalized using **softmax** to create **attention weights** that sum to 1.0. Finally, these weights are used to create a weighted sum of all the Value vectors, producing a context-aware representation of the original word.\n",
        "\n",
        "> The goal of the attention mechanism is to add contextual information to an input sequence\n",
        "{:.green}\n",
        "\n",
        "The beauty of self-attention is in its ability to capture long-range dependencies and contextual relationships. In the sentence \"The animal didn't cross the street because it was too tired,\" self-attention helps the model understand that \"it\" refers to \"animal\" rather than \"street\" by computing high attention weights between these tokens. The mechanism runs in parallel for all positions, making it highly efficient. Modern Transformers use **multi-head attention**, which means running multiple self-attention operations simultaneously with different learned weight matrices, allowing the model to attend to different types of relationships - one head might focus on syntactic structure, another on semantic meaning, and yet another on coreference resolution.\n",
        "\n",
        "![Check the GPU utilisation](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/self_attention_sketch.svg)\n",
        "\n",
        "Positional encodings are vectors **added** to the token embeddings.\n",
        "\n",
        "### Feed Forward Network (FFN) in Transformers\n",
        "\n",
        "The **Feed Forward Network (FFN)** is a crucial component that comes after the attention mechanism in each Transformer layer. While self-attention helps tokens communicate and share information with each other, the FFN processes each token's representation independently to capture complex, non-linear patterns.\n",
        "\n",
        "#### How FFN Works\n",
        "\n",
        "The FFN is applied to each position separately and identically, consisting of two linear transformations with a non-linear activation (ReLU or GELU) in between. The architecture follows a specific pattern: it first **expands** the dimensionality by a factor of 4 (sometimes 2 or 8 depending on the model), applies the non-linear activation, then **compresses** back down to the original embedding size.\n",
        "\n",
        "For example, in GPT-3 with a model dimension of 12,288, the FFN expands to 49,152 dimensions (4x) in the hidden layer, applies ReLU activation to introduce non-linearity, then projects back down to 12,288 dimensions. This expansion-compression pattern is sometimes called a \"bottleneck\" architecture, though here the \"bottleneck\" is actually the wider middle layer that allows the network to learn richer representations.\n",
        "\n",
        "![Feed Forward Network](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/ffn_sketch.svg)\n",
        "\n",
        "The key purpose is to introduce **non-linearity** into the model. While attention mechanisms perform linear operations (weighted sums), the FFN with its ReLU activation can learn complex, non-linear transformations. This allows the model to capture intricate relationships and patterns that linear operations alone cannot represent. The 4x expansion gives the network enough capacity to learn these complex functions before compressing the information back into the standard embedding size for the next layer.\n",
        "\n",
        "## Prompting\n",
        "\n",
        "There are 2 types of prompt to consider:\n",
        "\n",
        "1. Conversational oriented\n",
        "2. Task oriented\n",
        "\n",
        "Here the specific components which make the good prompt[^2]:\n",
        "\n",
        "1. Context\n",
        "2. Clear Instructions\n",
        "3. Persona\n",
        "4. Desired Output\n",
        "5. Structure using RAW Markdown\n",
        "6. Examples\n",
        "\n",
        "> Proper prompting maximize the quality of model outputs.\n",
        "> {:.green}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CK6vdy8qg610"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz_Xl72NU0Fg"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install transformers\n",
        "pip install sentencepiece\n",
        "pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "import os"
      ],
      "metadata": {
        "id": "BjsGPf3PV9eh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "dolly_pipeline = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "juDaZhW6VZsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8949972e"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the secret\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Set the environment variable for Hugging Face\n",
        "os.environ[\"HF_TOKEN\"] = hf_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "def show_py_file(filepath):\n",
        "    \"\"\"Display Python file contents as markdown code block\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Create markdown with python syntax highlighting\n",
        "        markdown_content = f\"```python\\n{content}\\n```\"\n",
        "        display(Markdown(markdown_content))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "def in_md(md_txt):\n",
        "    md_formated_txt = f\"--- Response -------<br/>{md_txt}<br/>-------------------\"\n",
        "    display(Markdown(md_formated_txt))"
      ],
      "metadata": {
        "id": "1S1m35GLV2mJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat with Dolly"
      ],
      "metadata": {
        "id": "PGFlVuNecg3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt):\n",
        "  response = dolly_pipeline(prompt)\n",
        "  in_md(response[0]['generated_text'])\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJ4e-i50bNfp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a single shot example:"
      ],
      "metadata": {
        "id": "Tk7E70Qt-hgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the meaning of life?\"\n",
        "chat(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "NhRJ6wVmdscP",
        "outputId": "bd1fb3c6-0c09-4e50-f500-267323bad2ee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "--- Response -------<br/>The meaning of life is what you make of it.<br/>-------------------"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a few shot example:"
      ],
      "metadata": {
        "id": "EU-PQnvE-ULF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "System:\n",
        "  cow - moo\n",
        "  dog - bark\n",
        "  cat - meow\n",
        "\n",
        "User:\n",
        "  duck\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "chat(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "C_I8sv7dnclb",
        "outputId": "af9ecb18-19cb-4fda-86ab-5e6bcb312bea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "--- Response -------<br/>User:\n  cow - moo\n  dog - bark\n  cat - meow\n  duck - quack<br/>-------------------"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a chain of thoughts with calculation, although Dolly doesn't capable of of doing complex calcualations:"
      ],
      "metadata": {
        "id": "mh1dO35d-pEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "System:\n",
        "You are Dolly, a helpful reasoning assistant.\n",
        "When solving problems, you follow these steps:\n",
        "1. Understand the question carefully.\n",
        "2. Break it into smaller steps.\n",
        "3. Think through the reasoning step by step.\n",
        "4. show the calculated steps\n",
        "5. Provide the final answer clearly at the end.\n",
        "\n",
        "User:\n",
        "If a train travels 60 km in 1.5 hours, what is its average speed in km/h?\n",
        "\n",
        "Assistant:\n",
        "Let's work through this step by step.\n",
        "\"\"\"\n",
        "\n",
        "chat(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "rf3b8podsMBz",
        "outputId": "f5edbcc9-d8db-409f-8134-981f79f717c5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "--- Response -------<br/>The user question: \nIf a train travels 60 km in 1.5 hours, what is its average speed in km/h?\n\nFirstly, you need to check if the given statement is correct. You confirm the 60km is correct by checking the equation. The hour should be 1.5 hours, right? Yes, the statement is correct.\n\nAssuming the train travels 60km in 1.5 hours, what is its average speed in km/h?\n\nIf we substitute 60km for X and 1.5 hours for t, the average speed will be:\nX/t = (60/1.5) = 30 km/h<br/>-------------------"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More complex chain of thoughts example:"
      ],
      "metadata": {
        "id": "fJXjGKtg-tMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "System:\n",
        "  You are Dolly, an expert assistant.\n",
        "  Provide concise, accurate answers for the question with a numbered, easy-to-follow step-by-step procedure that shows how to reach the answer.\n",
        "  Reveal internal chain-of-thought or inner monologue.\n",
        "\n",
        "User:\n",
        "  Example 1:\n",
        "  Calculate 23 Ã— 47.\n",
        "\n",
        "Assistant:\n",
        "  Answer: 1081\n",
        "  Steps:\n",
        "  1. Compute 23 Ã— 40 = 920.\n",
        "  2. Compute 23 Ã— 7 = 161.\n",
        "  3. Add 920 + 161 = 1081.\n",
        "  Final: 1081\n",
        "\n",
        "User:\n",
        "  Example 2:\n",
        "  Plan a 30-minute beginner running workout.\n",
        "\n",
        "Assistant:\n",
        "  Answer: 30-minute beginner run/walk workout.\n",
        "  Steps:\n",
        "  1. Warm up brisk walk for 5 minutes.\n",
        "  2. Alternate 2 minutes jogging / 1 minute walking for 18 minutes (6 cycles).\n",
        "  3. Cool down walk for 5 minutes and stretch.\n",
        "  Final: 5 min warmup, 18 min intervals, 5 min cooldown\n",
        "\n",
        "User:\n",
        "  Question:\n",
        "  How do I fix a pandas KeyError when accessing a DataFrame column?\n",
        "\n",
        "  Assistant:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "chat(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "mxNmTwiJ8vex",
        "outputId": "2b25bed8-92c9-4a6d-f7bf-267132c38680"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "--- Response -------<br/>The first question to address is whether or not there is a key error occurring. To troubleshoot this, you can perform a check to see if the DataFrame has any key variables by running the following code:\n\ndf.keys()\n\nIf a key variable is returned, you have likely introduced a key error via a column that does not have a key. If so, the following steps can be used to fix the key error:\n\n1. Identify the offending column:\n   - If the key exists and is a string, you can use the try_filter method to validate the string to make sure it exists in the key variable\n   - If the key exists and is a tuple, you can use the unique method to make sure the tuple is distinct and then use the dist function to see if it's in the key variable\n   - If the key variable does not contain any string, or tuple, then the DataFrame contains a no_keys DataType and you'll need to use a no_key method to create the missing value in the key variable\n\n2. Create the missing value:\n   - If the key variable contains a string, use the try_cast method to convert it to the type of the key variable<br/>-------------------"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning\n",
        "\n",
        "There are several neural networks:\n",
        "\n",
        "1. Recurrent neural networks (RNN)\n",
        "2. Convolutional neural networks (CNN)\n",
        "3. **Transformer-base models**\n",
        "\n",
        "Transformer-based LLMs are the standard in LLMs because of their\n",
        "\n",
        "- Parallelisation capabilities and\n",
        "- effectiveness in handling long-range dependencies in text.\n",
        "\n",
        "Fine-tuning LLMs with internal data enables these pre-trained models to understand language nuances and generate coherent responses, while aligning with domain-specific insights and knowledge.\n",
        "\n",
        "You can fine-tune AI FM on Amazon SageMaker AI for a specific problem domain, which optimises performance and conserves computational resources for that domain.\n",
        "\n",
        "CUDA (Compute Unified Device Architecture) is a proprietary and closed-source parallel computing platform and API to consume NVIDIA GPUs.\n",
        "\n",
        "To check the GPU memory:\n",
        "\n",
        "```bash\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "![Check the GPU utilisation](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/Check_the_GPU_utilisation.jpg)\n",
        "\n",
        "Let's install the Hugging Face Transformers library and the PyTorch library, which is a dependency for Transformers.\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "Here is the requirements.txt:\n",
        "\n",
        "```\n",
        "accelerate>=0.20.3,<1\n",
        "bitsandbytes==0.39.0\n",
        "click>=8.0.4,<9\n",
        "datasets>=2.10.0,<3\n",
        "deepspeed>=0.8.3,<0.9\n",
        "faiss-cpu==1.7.4\n",
        "ipykernel==6.22.0\n",
        "langchain==0.0.161\n",
        "torch>=1.13.1,<2\n",
        "transformers==4.28.1\n",
        "peft==0.3.0\n",
        "pytest==7.3.2\n",
        "numpy>=1.25.2\n",
        "scipy\n",
        "```\n",
        "\n",
        "To import PyTorch and Hugginface libraries:\n",
        "\n",
        "```python\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Ignore all warnings\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "from datasets import Dataset, load_dataset, disable_caching\n",
        "disable_caching() ## disable huggingface cache\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TextDataset\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "from IPython.display import Markdown\n",
        "```\n",
        "\n",
        "After importing the libraries, you can train the data:\n",
        "\n",
        "```python\n",
        "sagemaker_faqs_dataset = load_dataset(\"csv\",\n",
        "                                      data_files='data/amazon_sagemaker_faqs.csv')['train']\n",
        "sagemaker_faqs_dataset\n",
        "```\n",
        "\n",
        "Here is the output:\n",
        "\n",
        "![img000482@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000482@2x.jpg)\n",
        "\n",
        "To display the contents:\n",
        "\n",
        "```python\n",
        "sagemaker_faqs_dataset[0]\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "![img000483@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000483@2x.jpg)\n",
        "\n",
        "Decorate the instruction dataset with a PROMPT to fine tune the LLM:\n",
        "\n",
        "```python\n",
        "from utils.helpers import INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY, RESPONSE_KEY_NL, DEFAULT_SEED, PROMPT\n",
        "'''\n",
        "PROMPT = \"\"\"{intro}\n",
        "            {instruction_key}\n",
        "            {instruction}\n",
        "            {response_key}\n",
        "            {response}\n",
        "            {end_key}\"\"\"\n",
        "'''\n",
        "Markdown(PROMPT)\n",
        "```\n",
        "\n",
        "output\n",
        "\n",
        "```\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Response: {response} ### End\n",
        "```\n",
        "\n",
        "Using the `_add_text` Python function, feed the PROMPT to the dataset, which takes a record as input.\n",
        "\n",
        "The function checks that both fields (instruction/response) are not null, and then passes the values to the predefined PROMPT template above.\n",
        "\n",
        "```python\n",
        "def _add_text(rec):\n",
        "    instruction = rec[\"instruction\"]\n",
        "    response = rec[\"response\"]\n",
        "\n",
        "    if not instruction:\n",
        "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
        "\n",
        "    if not response:\n",
        "        raise ValueError(f\"Expected a response in: {rec}\")\n",
        "\n",
        "    rec[\"text\"] = PROMPT.format(\n",
        "        instruction=instruction, response=response)\n",
        "\n",
        "    return rec\n",
        "```\n",
        "\n",
        "You can create the template-based dataset as follows:\n",
        "\n",
        "```python\n",
        "sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(_add_text)\n",
        "sagemaker_faqs_dataset[0]\n",
        "```\n",
        "\n",
        "![img000485@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000485@2x.jpg)\n",
        "\n",
        "The above shows the output of the first element. If you convert this to markdown:\n",
        "\n",
        "```python\n",
        "Markdown(sagemaker_faqs_dataset[0]['text'])\n",
        "```\n",
        "\n",
        "![img000486@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000486@2x.jpg)\n",
        "\n",
        "### To load Pretrained LLM\n",
        "\n",
        "To load a pre-trained model, initialize a tokenizer and a base model by using the `databricks/dolly-v2-3b` model from the Hugging Face Transformers library. The tokenizer converts raw text into tokens, and the base model generates text based on a given prompt. By following the instructions previously outlined, you can correctly instantiate these components and use their functionality in your code.\n",
        "\n",
        "\n",
        "The `AutoTokenizer.from_pretrained()` Python function is used to instantiate the tokenizer.\n",
        "- `padding_side=\"left\"` specifies the side of the sequences where padding tokens are added. In this case, padding tokens are added to the left side of each sequence.\n",
        "- `eos_token` is a special token that represents the end of a sequence. By assigning the token to `pad_token`, any padding tokens added during tokenization are also considered end-of-sequence tokens. This can be useful when generating text through the model because the model knows when to stop generating text after encountering padding tokens.\n",
        "- `tokenizer.add_special_tokens...` adds three additional special tokens to the tokenizer's vocabulary. These tokens likely serve specific purposes in the application using the tokenizer. For example, the tokens could be used to mark the end of an input, an instruction, or a response in a dialogue system.\n",
        "\n",
        "After the function runs, the `tokenizer` object has been initialized and is ready to use for tokenizing text.\n",
        "\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\",\n",
        "                                          padding_side=\"left\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\":\n",
        "                              [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})\n",
        "```\n",
        "\n",
        "output\n",
        "\n",
        "![img000487@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000487@2x.jpg)\n",
        "\n",
        "create model\n",
        "\n",
        "```python\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"databricks/dolly-v2-3b\",\n",
        "    # use_cache=False,\n",
        "    device_map=\"auto\", #\"balanced\",\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "```\n",
        "\n",
        "![img000488@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000488@2x.jpg)\n",
        "\n",
        "### Prepare the Model for training\n",
        "\n",
        "Some preprocessing is needed before training an INT8 model using Parameter-Efficient Fine-Tuning (PEFT); therefore, import a utility function, `prepare_model_for_int8_training`, that will:\n",
        "\n",
        "- Cast all the non-INT8 modules to full precision (FP32) for stability.\n",
        "- Add a forward_hook to the input embedding layer to enable gradient computation of the input hidden states.\n",
        "- Enable gradient checkpointing for more memory-efficient training.\n",
        "\n",
        "```python\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "```\n",
        "\n",
        "output is `Embedding(50281, 2560)`.\n",
        "\n",
        "Use the `preprocess_batch` function to preprocess the text field of the batch, applying tokenization, truncation, and other relevant operations based on the specified maximum length. The field takes a batch of data, a tokenizer, and a maximum length as input.\n",
        "\n",
        "For more details, refer to `mlu_utils/helpers.py` file.\n",
        "\n",
        "```python\n",
        "from functools import partial\n",
        "from utils.helpers import mlu_preprocess_batch\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "_preprocessing_function = partial(mlu_preprocess_batch, max_length=MAX_LENGTH, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "Next, apply the preprocessing function to each batch in the dataset, modifying the text field accordingly. The map operation is performed in a batched manner, and the instruction, response, and text columns are removed from the dataset. Finally, `processed_dataset` is created by filtering `sagemaker_faqs_dataset` based on the length of the input_ids field, ensuring that it fits within the specified `MAX_LENGTH`.\n",
        "\n",
        "```python\n",
        "encoded_sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"instruction\", \"response\", \"text\"],\n",
        ")\n",
        "\n",
        "processed_dataset = encoded_sagemaker_faqs_dataset.filter(lambda rec: len(rec[\"input_ids\"]) < MAX_LENGTH)\n",
        "```\n",
        "\n",
        "![img000489@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000489@2x.jpg)\n",
        "\n",
        "### Split the dataset into `train` and `test`\n",
        "\n",
        "Split the dataset into `train` and `test` for evaluation.\n",
        "\n",
        "```python\n",
        "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
        "split_dataset\n",
        "```\n",
        "\n",
        "output is\n",
        "\n",
        "![img000490@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000490@2x.jpg)\n",
        "\n",
        "## Define the trainer and fine-tune the LLM\n",
        "\n",
        "To efficiently fine-tune a model, in this practice lab, you use [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685). LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and reduce the GPU memory requirement by 3 times.\n",
        "\n",
        "### Define LoraConfig and load the LoRA mode\n",
        "\n",
        "Use the build LoRA class `LoraConfig` from [huggingface ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft). Within `LoraConfig`, specify the following parameters:\n",
        "\n",
        "- `r`, the dimension of the low-rank matrices\n",
        "- `lora_alpha`, the scaling factor for the low-rank matrices\n",
        "- `lora_dropout`, the dropout probability of the LoRA layers\n",
        "\n",
        "```python\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
        "\n",
        "# First prepare the model for int8 training\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "# Then freeze all parameters\n",
        "for param in model.parameters():\n",
        "    if param.dtype == torch.float32 or param.dtype == torch.float16:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Unfreeze only the top N layers\n",
        "num_layers_to_unfreeze = 2  # Adjust this number as needed\n",
        "for i, layer in enumerate(model.gpt_neox.layers[-num_layers_to_unfreeze:]):\n",
        "    for param in layer.parameters():\n",
        "        if param.dtype == torch.float32 or param.dtype == torch.float16:\n",
        "            param.requires_grad = True\n",
        "        \n",
        "MICRO_BATCH_SIZE = 8\n",
        "BATCH_SIZE = 64\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "LORA_R = 256 # 512\n",
        "LORA_ALPHA = 512 # 1024\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "        \n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        "                 r=LORA_R,\n",
        "                 lora_alpha=LORA_ALPHA,\n",
        "                 lora_dropout=LORA_DROPOUT,\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "```\n",
        "\n",
        "Use the `get_peft_model` function to initialize the model with the LoRA framework, configuring it based on the provided `lora_config` settings. This way, the model can incorporate the benefits and capabilities of the LoRA optimization approach.\n",
        "\n",
        "```python\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "trainable params: 83886080 || all params: 2858977280 || trainable%: 2.9341289483769524\n",
        "```\n",
        "\n",
        "> As shown, LoRA-only trainable parameters are about 3 percent of the full weights, which is much more efficient.\n",
        "\n",
        "### Define the data collator\n",
        "\n",
        "A DataCollator is a huggingfaceðŸ¤— transformers function that takes a list of samples from a dataset and collates them into a batch, as a dictionary of PyTorch tensors.\n",
        "\n",
        "Use `DataCollatorForCompletionOnlyLM`, which extends the functionality of the base `DataCollatorForLanguageModeling` class from the Transformers library. This custom collator is designed to handle examples where a prompt is followed by a response in the input text and the labels are modified accordingly.\n",
        "\n",
        "For implementation, refer to `utils/helpers.py`.\n",
        "\n",
        "```python\n",
        "from utils.helpers import MLUDataCollatorForCompletionOnlyLM\n",
        "\n",
        "data_collator = MLUDataCollatorForCompletionOnlyLM(\n",
        "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Define the trainer\n",
        "\n",
        "To fine-tune the LLM, you must define a trainer. First, define the training arguments.\n",
        "\n",
        "```python\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
        "                    fp16=True,\n",
        "                    per_device_train_batch_size=8,\n",
        "                    per_device_eval_batch_size=8,\n",
        "                    learning_rate=LEARNING_RATE,\n",
        "                    num_train_epochs=EPOCHS,\n",
        "                    logging_strategy=\"steps\",\n",
        "                    logging_steps=100,\n",
        "                    evaluation_strategy=\"steps\",\n",
        "                    eval_steps=100,\n",
        "                    save_strategy=\"steps\",\n",
        "                    save_steps=20000,\n",
        "                    save_total_limit=10,\n",
        "                    optim=\"adamw_torch\"\n",
        ")\n",
        "```\n",
        "\n",
        "This is where the magic happens! Initialize the trainer with the defined model, tokenizer, training arguments, data collator, and the train/eval datasets.\n",
        "\n",
        "Here the hyperparameters:\n",
        "\n",
        "```python\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        train_dataset=split_dataset['train'],\n",
        "        eval_dataset=split_dataset[\"test\"],\n",
        "        data_collator=data_collator,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "![img000491@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000491@2x.jpg)\n",
        "\n",
        "Here the final output of the above:\n",
        "\n",
        "```\n",
        "TrainOutput(global_step=170, training_loss=0.8137160301208496, metrics={'train_runtime': 612.9883, 'train_samples_per_second': 2.17, 'train_steps_per_second': 0.277, 'total_flos': 4224342422077440.0, 'train_loss': 0.8137160301208496, 'epoch': 10.0})\n",
        "```\n",
        "\n",
        "### Save the fine-tuned model\n",
        "\n",
        "When the training is completed, you can save the model to a directory by using the [`transformers.PreTrainedModel.save_pretrained`] function. This function saves only the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, so the model is very efficient to store, transfer, and load.\n",
        "\n",
        "```python\n",
        "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
        "```\n",
        "\n",
        "Here `MODEL_SAVE_FOLDER_NAME` is `dolly-3b-lora`. If you want to save the full model that you just fine-tuned, you can use the [`transformers.trainer.save_model`] function. Meanwhile, save the training arguments together with the trained model.\n",
        "\n",
        "```python\n",
        "trainer.save_model()\n",
        "```\n",
        "\n",
        "```python\n",
        "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
        "```\n",
        "\n",
        "Save the tokenizer along with the trained model.\n",
        "\n",
        "```python\n",
        "tokenizer.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "![img000492@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000492@2x.jpg)\n",
        "\n",
        "## Deploy the fine-tuned model\n",
        "\n",
        "### Overview of deployment parameters\n",
        "\n",
        "To deploy using the Amazon SageMaker Python SDK with the Deep Java Library (DJL), you must instantiate the `Model` class with the following parameters:\n",
        "```{python}\n",
        "model = Model(\n",
        "    image_uri,\n",
        "    model_data=...,\n",
        "    predictor_cls=...,\n",
        "    role=aws_role\n",
        ")\n",
        "```\n",
        "- `image_uri`: The Docker image URI representing the deep learning framework and version to be used.\n",
        "- `model_data`: The location of the fine-tuned LLM model artifact in an Amazon Simple Storage Service (Amazon S3) bucket. It specifies the path to the TAR GZ file containing the model's parameters, architecture, and any necessary artifacts.\n",
        "- `predictor_cls`: This is just a JSON in JSON out predictor, nothing DJL related. For more information, see [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor).\n",
        "- `role`: The AWS Identity and Access Management (IAM) role ARN that provides necessary permissions to access resources, such as the S3 bucket that contains the model data.\n",
        "\n",
        "### Instantiate SageMaker parameters\n",
        "\n",
        "Initialize an Amazon SageMaker session and retrieve information related to the AWS environment such as the SageMaker role and AWS Region. You also specify the image URI for a specific version of the \"djl-deepspeed\" framework by using the SageMaker session's Region. The image URI is a unique identifier for a specific Docker container image that can be used in various AWS services, such as Amazon SageMaker or Amazon Elastic Container Registry (Amazon ECR).\n",
        "\n",
        "```bash\n",
        "pip3 install sagemaker==2.237.1\n",
        "```\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import json\n",
        "import sagemaker.djl_inference\n",
        "from sagemaker.session import Session\n",
        "from sagemaker import image_uris\n",
        "from sagemaker import Model\n",
        "\n",
        "sagemaker_session = Session()\n",
        "print(\"sagemaker_session: \", sagemaker_session)\n",
        "\n",
        "aws_role = sagemaker_session.get_caller_identity_arn()\n",
        "print(\"aws_role: \", aws_role)\n",
        "\n",
        "aws_region = boto3.Session().region_name\n",
        "print(\"aws_region: \", aws_region)\n",
        "\n",
        "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\",\n",
        "                                version=\"0.22.1\",\n",
        "                                region=sagemaker_session._region_name)\n",
        "print(\"image_uri: \", image_uri)\n",
        "```\n",
        "\n",
        "![img000494@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000494@2x.jpg)\n",
        "\n",
        "### Create the model artifact\n",
        "\n",
        "To upload the model artifact to the S3 bucket, we need to create a TAR GZ file that contains the model's parameters. First, create a directory named `lora_model` and a subdirectory named `dolly-3b-lora`. The \"-p\" option ensures that the command creates any intermediate directories if they don't exist. Then, copy the LoRA checkpoints, `adapter_model.bin` and `adapter_config.json`, to `dolly-3b-lora`. The base Dolly model is downloaded at runtime from the Hugging Face Hub.\n",
        "\n",
        "```bash\n",
        "rm -rf lora_model\n",
        "mkdir -p lora_model\n",
        "mkdir -p lora_model/dolly-3b-lora\n",
        "cp dolly-3b-lora/adapter_config.json lora_model/dolly-3b-lora/\n",
        "cp dolly-3b-lora/adapter_model.bin lora_model/dolly-3b-lora/\n",
        "```\n",
        "\n",
        "Next, set the [DJL Serving configuration options](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) in `serving.properties`. Using the Jupyter `%%writefile` magic command, you can write the following content to a file named lora_model/serving.properties.\n",
        "\n",
        "- `engine=Python`: This line specifies the engine used for serving.\n",
        "- `option.entryPoint=model.py`: This line specifies the entry point for the serving process, which is set to model.py.\n",
        "- `option.adapter_checkpoint=dolly-3b-lora`: This line sets the checkpoint for the adapter to dolly-3b-lora. A checkpoint typically represents the saved state of a model or its parameters.\n",
        "- `option.adapter_name=dolly-lora`: This line sets the name of the adapter to dolly-lora, a component that helps interface between the model and the serving infrastructure.\n",
        "\n",
        "```python\n",
        "%%writefile lora_model/serving.properties\n",
        "engine=Python\n",
        "option.entryPoint=model.py\n",
        "option.adapter_checkpoint=dolly-3b-lora\n",
        "option.adapter_name=dolly-lora\n",
        "```\n",
        "\n",
        "You also need the environment requirement file in the model artifact. Create a file named `lora_model/requirements.txt` and write a list of Python package requirements, typically used with package managers such as `pip`.\n",
        "\n",
        "```\n",
        "%%writefile lora_model/requirements.txt\n",
        "accelerate>=0.16.0,<1\n",
        "bitsandbytes==0.39.0\n",
        "click>=8.0.4,<9\n",
        "datasets>=2.10.0,<3\n",
        "deepspeed>=0.8.3,<0.9\n",
        "faiss-cpu==1.7.4\n",
        "ipykernel==6.22.0\n",
        "scipy==1.11.1\n",
        "torch>=2.0.0\n",
        "transformers==4.28.1\n",
        "peft==0.3.0\n",
        "pytest==7.3.2\n",
        "```\n",
        "\n",
        "### Create the inference script\n",
        "\n",
        "Similar to the fine-tuning notebook, a custom pipeline, `InstructionTextGenerationPipeline`, is defined. The code is provided in `utils/deployment_model.py`.\n",
        "\n",
        "You save these inference functions to `lora_model/model.py`.\n",
        "\n",
        "### Upload the model artifact to Amazon S3\n",
        "\n",
        "Create a compressed tarball archive of the lora_model directory and save it as lora_model.tar.gz.\n",
        "\n",
        "```\n",
        "%%bash\n",
        "tar -cvzf lora_model.tar.gz lora_model/\n",
        "```\n",
        "\n",
        "![img000493@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000493@2x.jpg)\n",
        "\n",
        "Upload the lora_model.tar.gz file to the specified S3 bucket.\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import json\n",
        "import sagemaker.djl_inference\n",
        "from sagemaker.session import Session\n",
        "from sagemaker import image_uris\n",
        "from sagemaker import Model\n",
        "\n",
        "s3 = boto3.resource('s3')\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "s3 = boto3.resource('s3')\n",
        "\n",
        "# Get the name of the bucket with prefix lab-code\n",
        "for bucket in s3.buckets.all():\n",
        "    if bucket.name.startswith('artifact'):\n",
        "        mybucket = bucket.name\n",
        "        print(mybucket)\n",
        "\n",
        "response = s3_client.upload_file(\"lora_model.tar.gz\", mybucket, \"lora_model.tar.gz\")\n",
        "```\n",
        "\n",
        "output is `artifact-f9d649b0`.\n",
        "\n",
        "### Deploy the model\n",
        "\n",
        "Now, it's the time to deploy the fine-tuned LLM by using the SageMaker Python SDK. The SageMaker Python SDK `Model` class is instantiated with the following parameters:\n",
        "\n",
        "- `image_uri`: The Docker image URI that represents the deep learning framework and version to be used.\n",
        "- `model_data`: The location of the fine-tuned LLM model artifact in an S3 bucket. It specifies the path to the TAR GZ file that contains the model's parameters, architecture, and any necessary artifacts.\n",
        "- `predictor_cls`: This is just a JSON in JSON out predictor, nothing DJL related. For more information, see [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor).\n",
        "- `role`: The IAM role ARN that provides necessary permissions to access resources, such as the S3 bucket that contains the model data.\n",
        "\n",
        "> The deployment should be completed within 10 minutes. Any longer than that, your endpoint might have failed.\n",
        "\n",
        "```python\n",
        "%%time\n",
        "predictor = model.deploy(1, \"ml.g4dn.2xlarge\")\n",
        "```\n",
        "\n",
        "![img000495@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000495@2x.jpg)\n",
        "\n",
        "### Test the deployed inference\n",
        "\n",
        "Test the inference endpoint with [predictor.predict](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.predict).\n",
        "\n",
        "```python\n",
        "outputs = predictor.predict({\"inputs\": \"What security measures does Amazon SageMaker have?\"})\n",
        "```\n",
        "\n",
        "```python\n",
        "from IPython.display import Markdown\n",
        "Markdown(outputs)\n",
        "```\n",
        "\n",
        "![img000496@2x](https://raw.githubusercontent.com/ojitha/blog/master/assets/images/2025-07-23-LLMonAWS/img000496@2x.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[^1]: Fluent Python, 2nd Edition, Luciano Ramalho, [Chapter 7](https://learning.oreilly.com/library/view/fluent-python-2nd/9781492056348/ch07.html#attrgetter_demo)\n",
        "[^2]: [AI for Nonprofits: Prompting 101 on Vimeo](https://vimeo.com/1101761957?autoplay=1&muted=1&stream_id=Y2xpcHN8NjY2OTMwMDZ8aWQ6ZGVzY3xbXQ%3D%3D)\n",
        "[^3]: [ChatGPT Tokeniser](https://platform.openai.com/tokenizer)\n",
        "[^4]: [Embedding projector - visualization of high-dimensional data](https://projector.tensorflow.org/)"
      ],
      "metadata": {
        "id": "kVxpuiZahb5k"
      }
    }
  ]
}