{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+CrGM1+9aHdv3/NWkYIRC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "layout: post\n",
        "title: LLM Tuning\n",
        "date:   2025-09-18\n",
        "categories: [Python]\n",
        "mermaid: true\n",
        "typora-root-url: /Users/ojitha/GitHub/ojitha.github.io\n",
        "typora-copy-images-to: ../../blog/assets/images/${filename}\n",
        "---"
      ],
      "metadata": {
        "id": "NTRURJivpVLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and Decoder\n",
        "A transformer is built on an encoder-decoder architecture, where the encoder takes in input and outputs a matrix representation of that input. The decoder iteratively generates output using that representation. Transformer[^1] relies on self-attention to model the relationship between tokens in a sentence.\n",
        "\n",
        "![Transformer](https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Transformer%2C_full_architecture.png/500px-Transformer%2C_full_architecture.png)\n",
        "\n",
        "- Transformer Encoder:\n",
        "  - Processes input sequence all at once (parallel processing)\n",
        "  - Uses self-attention to understand relationships between all words\n",
        "  - Output: Rich contextual representations of input\n",
        "- Transformer Decoder:\n",
        "  - Generates output sequence one token at a time (autoregressive)\n",
        "  - Uses self-attention + cross-attention to input encoder representations\n",
        "  - Output: Generated sequence (translation, text generation, etc.)\n",
        "\n",
        "There are 2 types of models:\n",
        "1. Auto-Regressive Models:\n",
        "  - What: Predict next token based on previous tokens\n",
        "  - Training: Learn $P(x_t | x_1, x_2, ..., x_{t-1})$\n",
        "  - Direction: Left-to-right (forward prediction)\n",
        "  - Examples: GPT, Claude, traditional language models\n",
        "  - Use cases: Text generation, completion, dialogue\n",
        "```\n",
        "Input:  \"The cat sat on\" → Predict: \"the\"\n",
        "```\n",
        "2. Auto-Encoding Models:\n",
        "  - What: Reconstruct corrupted/masked input\n",
        "  - Training: Learn to fill in missing pieces from context\n",
        "  - Direction: Bidirectional (sees both left and right context)\n",
        "  - Examples: BERT, RoBERTa, DeBERTa\n",
        "  - Use cases: Text understanding, classification, Q&A  \n",
        "```\n",
        "Input:  \"The [MASK] sat on mat\" → Predict: \"cat\"\n",
        "```\n",
        "\n",
        "Self-Attention: Each word \"looks at\" all other words to understand context and relationships.  \n",
        "\n",
        "![Autoregressive Models](/../blog/assets/images/2025_09_18_LLMTuningWithPyTorch/autoregressive_models.svg)\n",
        "\n",
        "> GPT and BERT are Transformers, but they are different language models.\n",
        "{:.info-box}"
      ],
      "metadata": {
        "id": "7OtQfyB3iSZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attenstion\n",
        "Standard attention calculated using 3 matrices:\n",
        "\n",
        "1. query\n",
        "2. key\n",
        "3. value"
      ],
      "metadata": {
        "id": "YcHJNsb387kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SQzSzESNzwj"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-dimensional tensor"
      ],
      "metadata": {
        "id": "2f7FClZVPajj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_d_tensor = torch.LongTensor([0])\n",
        "print(f'Shape of {one_d_tensor} is {one_d_tensor.shape} and dimension is {one_d_tensor.dim()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gochs8VPRwI",
        "outputId": "47f1501f-3e0f-48f1-fa77-779da9eabfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor([0]) is torch.Size([1]) and dimension is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_d_tensor = torch.LongTensor([0,1,2])\n",
        "print(f'Shape of {one_d_tensor} is {one_d_tensor.shape} and dimension is {one_d_tensor.dim()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb-BqznJP7M3",
        "outputId": "ae017224-3ce4-48ba-e6ff-ea4471a7dffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor([0, 1, 2]) is torch.Size([3]) and dimension is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_d_tensor = torch.LongTensor([[0,1,2],[3,4,5]])\n",
        "print(two_d_tensor.shape)\n",
        "print(f'Shape of {two_d_tensor} is {two_d_tensor.shape} and dimension is {two_d_tensor.dim()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihnENyH6QIlH",
        "outputId": "aad6ecb2-daea-44fd-e93a-f5a4205726f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "Shape of tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) is torch.Size([2, 3]) and dimension is 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_d_tensor = torch.LongTensor([0,1,2])\n",
        "two_d_tensor = one_d_tensor.unsqueeze(0)\n",
        "print(f'Shape of {two_d_tensor} is {two_d_tensor.shape} and dimension is {two_d_tensor.dim()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T37Nag_Qt-K",
        "outputId": "a458fc78-662e-43d3-fc14-52b5fe063ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor([[0, 1, 2]]) is torch.Size([1, 3]) and dimension is 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_d_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUP1VxwrSNUO",
        "outputId": "11f29bff-c12c-495f-8870-9a8073d2d252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_d_tensor.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSxmSdRRSx1L",
        "outputId": "299c7629-74c4-423b-fbe0-b2f1240f9430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [2]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Product development Lifecycle\n",
        "\n",
        "```mermaid\n",
        "block-beta\n",
        "    columns 5\n",
        "    \n",
        "    A[\"1. Prototype\"]:1\n",
        "    B[\"2. Evals\"]:1\n",
        "    C[\"3. Maximize\"]:1\n",
        "    D[\"4. Optimize\"]:1\n",
        "    E[\"5. Did you\n",
        "    solve it?\"]:1\n",
        "    \n",
        "    A1[\"Where does\n",
        "    AI fit in?\"]:1\n",
        "    B1[\"Define evals\"]:1\n",
        "    C1[\"Prompt engineering\"]:1\n",
        "    D1[\"Latency\n",
        "    and/or cost\"]:1\n",
        "    E1[\"Don't give up,\n",
        "    keep going!\"]:1\n",
        "    \n",
        "    A2[\"Copilot vs agent?\"]:1\n",
        "    B2[\"Make dataset\"]:1\n",
        "    C2[\"CoT, self-reflection,\n",
        "    strong models\"]:1\n",
        "    D2[\"Move tasks to\n",
        "    smaller models\"]:1\n",
        "    E2[\"Otherwise add\n",
        "    HITL until\"]:1\n",
        "    \n",
        "    A3[\"What's the\n",
        "    MVP experience?\"]:1\n",
        "    B3[\"Establish\n",
        "    baseline\"]:1\n",
        "    C3[\"RAG\n",
        "    improvements\"]:1\n",
        "    D3[\"Make\n",
        "    prompts/outputs\n",
        "    smaller\"]:1\n",
        "    E3[\"next-gen models\n",
        "    or rethink\"]:1\n",
        "    \n",
        "    A4[\"How good does\n",
        "    AI need to be?\"]:1\n",
        "    B4[\"Set a goal\"]:1\n",
        "    C4[\"Fine-tuning\n",
        "    and distillation\"]:1\n",
        "    D4[\"Fine-tune\n",
        "    as needed\"]:1\n",
        "    E4[\"your product\"]:1\n",
        "    \n",
        "    space:1\n",
        "    space:1\n",
        "    C5[\"Multi-agent\"]:1\n",
        "    space:1\n",
        "    space:1\n",
        "    \n",
        "    space:1\n",
        "    space:1\n",
        "    C6[\"Tools\"]:1\n",
        "    space:1\n",
        "    space:1\n",
        "    \n",
        "    A --> B\n",
        "    B --> C\n",
        "    C --> D\n",
        "    D --> E\n",
        "    \n",
        "    style A fill:#e1f5fe\n",
        "    style B fill:#f3e5f5\n",
        "    style C fill:#e8f5e8\n",
        "    style D fill:#fff3e0\n",
        "    style E fill:#fce4ec\n",
        "```"
      ],
      "metadata": {
        "id": "p8dLbIgiqie2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[^1]: [Attetnsion All You need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf){:target=\"_blank\"}\n",
        "\n",
        "\n",
        "{:gtxt: .message color=\"green\"}\n",
        "\n",
        "{:ytxt: .message color=\"yellow\"}\n",
        "\n",
        "{:rtxt: .message color=\"red\"}"
      ],
      "metadata": {
        "id": "qgbG4HwY7GNm"
      }
    }
  ]
}